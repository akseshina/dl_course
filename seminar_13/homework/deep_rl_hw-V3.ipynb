{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gpuarray): pygpu was configured but could not be imported or is too old (version 0.7 or higher required)\n",
      "NoneType\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda*\"  \n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing keras Dense (fully connected) layer and Sequential model\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=True):\n",
    "        # Tip: if you are training this on AWS the best way is to turn off rendering\n",
    "        # and load it later with the serialized model\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # Use tflearn to get simple NN for deep q-learning\n",
    "        # Spoler alert: a couple of fully connected hidden layers should be enough\n",
    "        # Output layer should have the same dimensionality as the action space\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim = self.state_size, activation='linear', init='lecun_uniform'))\n",
    "        model.add(Dense(64, activation='relu', init='lecun_uniform'))\n",
    "        model.add(Dense(32, activation='relu', init='lecun_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', init='lecun_uniform'))\n",
    "        model.compile(optimizer=RMSprop(lr=self.learning_rate), loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            # print(len(self.memory))\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                          np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # You can create a minibatch of the correct target answer and the current value of your own,\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_model(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/margarita/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"linear\", kernel_initializer=\"lecun_uniform\", input_dim=2)`\n",
      "/home/margarita/.local/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/home/margarita/.local/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"lecun_uniform\")`\n",
      "/home/margarita/.local/lib/python3.5/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", kernel_initializer=\"lecun_uniform\")`\n",
      "/home/margarita/.local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"linear\", kernel_initializer=\"lecun_uniform\", input_dim=2)`\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "ACTION_SIZE = 2\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE)\n",
    "# agent.load_model(\"./save_model/<your_saved_model_name>\")\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53583677  0.        ]]\n",
      "episode: 0   score: -200.0   memory length: 200   epsilon: 0.9960200000000077\n",
      "[[-0.51427713  0.        ]]\n",
      "episode: 1   score: -200.0   memory length: 400   epsilon: 0.9920400000000154\n",
      "[[-0.49273763  0.        ]]\n",
      "episode: 2   score: -200.0   memory length: 600   epsilon: 0.988060000000023\n",
      "[[-0.51529317  0.        ]]\n",
      "episode: 3   score: -200.0   memory length: 800   epsilon: 0.9840800000000307\n",
      "[[-0.40901322  0.        ]]\n",
      "episode: 4   score: -200.0   memory length: 1000   epsilon: 0.9801000000000384\n",
      "[[-0.54091158  0.        ]]\n",
      "episode: 5   score: -200.0   memory length: 1200   epsilon: 0.9761200000000461\n",
      "[[-0.54000392  0.        ]]\n",
      "episode: 6   score: -200.0   memory length: 1400   epsilon: 0.9721400000000537\n",
      "[[-0.57237521  0.        ]]\n",
      "episode: 7   score: -200.0   memory length: 1600   epsilon: 0.9681600000000614\n",
      "[[-0.41698564  0.        ]]\n",
      "episode: 8   score: -200.0   memory length: 1800   epsilon: 0.9641800000000691\n",
      "[[-0.47841907  0.        ]]\n",
      "episode: 9   score: -200.0   memory length: 2000   epsilon: 0.9602000000000768\n",
      "[[-0.48101037  0.        ]]\n",
      "episode: 10   score: -200.0   memory length: 2200   epsilon: 0.9562200000000844\n",
      "[[-0.4009593  0.       ]]\n",
      "episode: 11   score: -200.0   memory length: 2400   epsilon: 0.9522400000000921\n",
      "[[-0.54627004  0.        ]]\n",
      "episode: 12   score: -200.0   memory length: 2600   epsilon: 0.9482600000000998\n",
      "[[-0.57592189  0.        ]]\n",
      "episode: 13   score: -200.0   memory length: 2800   epsilon: 0.9442800000001075\n",
      "[[-0.55303378  0.        ]]\n",
      "episode: 14   score: -200.0   memory length: 3000   epsilon: 0.9403000000001152\n",
      "[[-0.51921346  0.        ]]\n",
      "episode: 15   score: -200.0   memory length: 3200   epsilon: 0.9363200000001228\n",
      "[[-0.58209287  0.        ]]\n",
      "episode: 16   score: -200.0   memory length: 3400   epsilon: 0.9323400000001305\n",
      "[[-0.51444356  0.        ]]\n",
      "episode: 17   score: -200.0   memory length: 3600   epsilon: 0.9283600000001382\n",
      "[[-0.48856965  0.        ]]\n",
      "episode: 18   score: -200.0   memory length: 3800   epsilon: 0.9243800000001459\n",
      "[[-0.52592939  0.        ]]\n",
      "episode: 19   score: -200.0   memory length: 4000   epsilon: 0.9204000000001535\n",
      "[[-0.40226442  0.        ]]\n",
      "episode: 20   score: -200.0   memory length: 4200   epsilon: 0.9164200000001612\n",
      "[[-0.56113105  0.        ]]\n",
      "episode: 21   score: -200.0   memory length: 4400   epsilon: 0.9124400000001689\n",
      "[[-0.41590624  0.        ]]\n",
      "episode: 22   score: -200.0   memory length: 4600   epsilon: 0.9084600000001766\n",
      "[[-0.58668832  0.        ]]\n",
      "episode: 23   score: -200.0   memory length: 4800   epsilon: 0.9044800000001842\n",
      "[[-0.57197046  0.        ]]\n",
      "episode: 24   score: -200.0   memory length: 5000   epsilon: 0.9005000000001919\n",
      "[[-0.58796145  0.        ]]\n",
      "episode: 25   score: -200.0   memory length: 5200   epsilon: 0.8965200000001996\n",
      "[[-0.54007079  0.        ]]\n",
      "episode: 26   score: -200.0   memory length: 5400   epsilon: 0.8925400000002073\n",
      "[[-0.55405352  0.        ]]\n",
      "episode: 27   score: -200.0   memory length: 5600   epsilon: 0.888560000000215\n",
      "[[-0.45043119  0.        ]]\n",
      "episode: 28   score: -200.0   memory length: 5800   epsilon: 0.8845800000002226\n",
      "[[-0.41990999  0.        ]]\n",
      "episode: 29   score: -200.0   memory length: 6000   epsilon: 0.8806000000002303\n",
      "[[-0.43730989  0.        ]]\n",
      "episode: 30   score: -200.0   memory length: 6200   epsilon: 0.876620000000238\n",
      "[[-0.50100287  0.        ]]\n",
      "episode: 31   score: -200.0   memory length: 6400   epsilon: 0.8726400000002457\n",
      "[[-0.49338285  0.        ]]\n",
      "episode: 32   score: -200.0   memory length: 6600   epsilon: 0.8686600000002533\n",
      "[[-0.4032447  0.       ]]\n",
      "episode: 33   score: -200.0   memory length: 6800   epsilon: 0.864680000000261\n",
      "[[-0.44586296  0.        ]]\n",
      "episode: 34   score: -200.0   memory length: 7000   epsilon: 0.8607000000002687\n",
      "[[-0.59018498  0.        ]]\n",
      "episode: 35   score: -200.0   memory length: 7200   epsilon: 0.8567200000002764\n",
      "[[-0.49603244  0.        ]]\n",
      "episode: 36   score: -200.0   memory length: 7400   epsilon: 0.852740000000284\n",
      "[[-0.54446517  0.        ]]\n",
      "episode: 37   score: -200.0   memory length: 7600   epsilon: 0.8487600000002917\n",
      "[[-0.52823535  0.        ]]\n",
      "episode: 38   score: -200.0   memory length: 7800   epsilon: 0.8447800000002994\n",
      "[[-0.51298533  0.        ]]\n",
      "episode: 39   score: -200.0   memory length: 8000   epsilon: 0.8408000000003071\n",
      "[[-0.41050116  0.        ]]\n",
      "episode: 40   score: -200.0   memory length: 8200   epsilon: 0.8368200000003148\n",
      "[[-0.42597386  0.        ]]\n",
      "episode: 41   score: -200.0   memory length: 8400   epsilon: 0.8328400000003224\n",
      "[[-0.55355716  0.        ]]\n",
      "episode: 42   score: -200.0   memory length: 8600   epsilon: 0.8288600000003301\n",
      "[[-0.56754626  0.        ]]\n",
      "episode: 43   score: -200.0   memory length: 8800   epsilon: 0.8248800000003378\n",
      "[[-0.50842845  0.        ]]\n",
      "episode: 44   score: -200.0   memory length: 9000   epsilon: 0.8209000000003455\n",
      "[[-0.55137717  0.        ]]\n",
      "episode: 45   score: -200.0   memory length: 9200   epsilon: 0.8169200000003531\n",
      "[[-0.53398167  0.        ]]\n",
      "episode: 46   score: -200.0   memory length: 9400   epsilon: 0.8129400000003608\n",
      "[[-0.40767622  0.        ]]\n",
      "episode: 47   score: -200.0   memory length: 9600   epsilon: 0.8089600000003685\n",
      "[[-0.48439841  0.        ]]\n",
      "episode: 48   score: -200.0   memory length: 9800   epsilon: 0.8049800000003762\n",
      "[[-0.47099825  0.        ]]\n",
      "episode: 49   score: -200.0   memory length: 10000   epsilon: 0.8010000000003838\n",
      "[[-0.57954626  0.        ]]\n",
      "episode: 50   score: -200.0   memory length: 10000   epsilon: 0.7970200000003915\n",
      "[[-0.49926608  0.        ]]\n",
      "episode: 51   score: -200.0   memory length: 10000   epsilon: 0.7930400000003992\n",
      "[[-0.49412279  0.        ]]\n",
      "episode: 52   score: -200.0   memory length: 10000   epsilon: 0.7890600000004069\n",
      "[[-0.53672628  0.        ]]\n",
      "episode: 53   score: -200.0   memory length: 10000   epsilon: 0.7850800000004146\n",
      "[[-0.43159426  0.        ]]\n",
      "episode: 54   score: -200.0   memory length: 10000   epsilon: 0.7811000000004222\n",
      "[[-0.54554478  0.        ]]\n",
      "episode: 55   score: -200.0   memory length: 10000   epsilon: 0.7771200000004299\n",
      "[[-0.45349228  0.        ]]\n",
      "episode: 56   score: -200.0   memory length: 10000   epsilon: 0.7731400000004376\n",
      "[[-0.49099491  0.        ]]\n",
      "episode: 57   score: -200.0   memory length: 10000   epsilon: 0.7691600000004453\n",
      "[[-0.57888741  0.        ]]\n",
      "episode: 58   score: -200.0   memory length: 10000   epsilon: 0.7651800000004529\n",
      "[[-0.44762946  0.        ]]\n",
      "episode: 59   score: -200.0   memory length: 10000   epsilon: 0.7612000000004606\n",
      "[[-0.48877814  0.        ]]\n",
      "episode: 60   score: -200.0   memory length: 10000   epsilon: 0.7572200000004683\n",
      "[[-0.43510342  0.        ]]\n",
      "episode: 61   score: -200.0   memory length: 10000   epsilon: 0.753240000000476\n",
      "[[-0.52402513  0.        ]]\n",
      "episode: 62   score: -200.0   memory length: 10000   epsilon: 0.7492600000004837\n",
      "[[-0.47118868  0.        ]]\n",
      "episode: 63   score: -200.0   memory length: 10000   epsilon: 0.7452800000004913\n",
      "[[-0.56399676  0.        ]]\n",
      "episode: 64   score: -200.0   memory length: 10000   epsilon: 0.741300000000499\n",
      "[[-0.42158939  0.        ]]\n",
      "episode: 65   score: -200.0   memory length: 10000   epsilon: 0.7373200000005067\n",
      "[[-0.47666474  0.        ]]\n",
      "episode: 66   score: -200.0   memory length: 10000   epsilon: 0.7333400000005144\n",
      "[[-0.4025322  0.       ]]\n",
      "episode: 67   score: -200.0   memory length: 10000   epsilon: 0.729360000000522\n",
      "[[-0.40186572  0.        ]]\n",
      "episode: 68   score: -200.0   memory length: 10000   epsilon: 0.7253800000005297\n",
      "[[-0.44114639  0.        ]]\n",
      "episode: 69   score: -200.0   memory length: 10000   epsilon: 0.7214000000005374\n",
      "[[-0.49575583  0.        ]]\n",
      "episode: 70   score: -200.0   memory length: 10000   epsilon: 0.7174200000005451\n",
      "[[-0.55105772  0.        ]]\n",
      "episode: 71   score: -200.0   memory length: 10000   epsilon: 0.7134400000005527\n",
      "[[-0.55503067  0.        ]]\n",
      "episode: 72   score: -200.0   memory length: 10000   epsilon: 0.7094600000005604\n",
      "[[-0.52340394  0.        ]]\n",
      "episode: 73   score: -200.0   memory length: 10000   epsilon: 0.7054800000005681\n",
      "[[-0.40468782  0.        ]]\n",
      "episode: 74   score: -200.0   memory length: 10000   epsilon: 0.7015000000005758\n",
      "[[-0.51053962  0.        ]]\n",
      "episode: 75   score: -200.0   memory length: 10000   epsilon: 0.6975200000005835\n",
      "[[-0.48131336  0.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 76   score: -200.0   memory length: 10000   epsilon: 0.6935400000005911\n",
      "[[-0.48214496  0.        ]]\n",
      "episode: 77   score: -200.0   memory length: 10000   epsilon: 0.6895600000005988\n",
      "[[-0.49975231  0.        ]]\n",
      "episode: 78   score: -200.0   memory length: 10000   epsilon: 0.6855800000006065\n",
      "[[-0.59734485  0.        ]]\n",
      "episode: 79   score: -200.0   memory length: 10000   epsilon: 0.6816000000006142\n",
      "[[-0.40969727  0.        ]]\n",
      "episode: 80   score: -200.0   memory length: 10000   epsilon: 0.6776200000006218\n",
      "[[-0.54055586  0.        ]]\n",
      "episode: 81   score: -200.0   memory length: 10000   epsilon: 0.6736400000006295\n",
      "[[-0.41712782  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    print(state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        #if agent.render:\n",
    "        #    env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 4:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model(\"save_model/my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
